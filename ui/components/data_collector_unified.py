"""
ãƒ‡ãƒ¼ã‚¿åé›†UIï¼ˆçµ±åˆç‰ˆï¼‰

ãƒ‡ãƒ¼ã‚¿å–å¾—ä½œæ¥­ã‚’ä¸€å…ƒç®¡ç†:
- ã‚¯ã‚¤ãƒƒã‚¯åé›†ï¼ˆä»Šæ—¥/æ˜æ—¥/ä»Šé€±ï¼‰
- ä¸è¶³ãƒ‡ãƒ¼ã‚¿æ¤œå‡ºãƒ»å–å¾—ï¼ˆæœŸé–“æŒ‡å®šå¯¾å¿œï¼‰
- ã‚ªãƒªã‚¸ãƒŠãƒ«å±•ç¤ºåé›†

ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¸ãƒ§ãƒ–å¯¾å¿œç‰ˆ
"""
import streamlit as st
import subprocess
import os
import sys
import json
from datetime import datetime, timedelta
import sqlite3
from typing import List, Dict
import time

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.insert(0, PROJECT_ROOT)

from config.settings import DATABASE_PATH, VENUES
from src.utils.job_manager import (
    is_job_running, start_job, get_job_progress,
    cancel_job, get_all_jobs
)
from src.analysis.data_coverage_checker import DataCoverageChecker

# ã‚¸ãƒ§ãƒ–åå®šæ•°
JOB_TENJI = 'tenji_collection'
JOB_MISSING_DATA = 'missing_data_fetch'
JOB_DATA_COLLECTION = 'data_collection'


def render_data_collector():
    """ãƒ‡ãƒ¼ã‚¿åé›†UIã®ãƒ¡ã‚¤ãƒ³ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°"""
    st.header("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿åé›†")

    # å®Ÿè¡Œä¸­ã‚¸ãƒ§ãƒ–ã®çŠ¶æ³è¡¨ç¤º
    _render_job_status_bar()

    st.markdown("ãƒ‡ãƒ¼ã‚¿å–å¾—ä½œæ¥­ã‚’ä¸€å…ƒç®¡ç†ã—ã¾ã™ã€‚ã‚¿ãƒ–ã§ä½œæ¥­ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")

    tab1, tab2, tab3 = st.tabs([
        "ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯åé›†",
        "ğŸ” ä¸è¶³ãƒ‡ãƒ¼ã‚¿æ¤œå‡º",
        "ğŸ¯ ã‚ªãƒªã‚¸ãƒŠãƒ«å±•ç¤º"
    ])

    with tab1:
        _render_quick_collection()

    with tab2:
        _render_missing_data_detector()

    with tab3:
        _render_original_tenji()


def _render_job_status_bar():
    """å®Ÿè¡Œä¸­ã‚¸ãƒ§ãƒ–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤º"""
    jobs = get_all_jobs()
    running_jobs = {k: v for k, v in jobs.items() if v.get('is_running')}

    if running_jobs:
        st.info("ğŸ”„ **ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¸ãƒ§ãƒ–å®Ÿè¡Œä¸­** - ã‚¿ãƒ–ã‚’ç§»å‹•ã—ã¦ã‚‚å‡¦ç†ã¯ç¶™ç¶šã—ã¾ã™")

        for job_name, progress in running_jobs.items():
            col1, col2, col3 = st.columns([3, 1, 1])

            with col1:
                job_label = {
                    JOB_TENJI: 'ã‚ªãƒªã‚¸ãƒŠãƒ«å±•ç¤ºåé›†',
                    JOB_MISSING_DATA: 'ä¸è¶³ãƒ‡ãƒ¼ã‚¿å–å¾—'
                }.get(job_name, job_name)

                progress_val = progress.get('progress', 0)
                message = progress.get('message', 'å‡¦ç†ä¸­...')

                st.progress(progress_val / 100, text=f"{job_label}: {message}")

            with col2:
                st.caption(f"é€²æ—: {progress_val}%")

            with col3:
                if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", key=f"dc_cancel_{job_name}"):
                    cancel_job(job_name)
                    st.rerun()

        if st.button("ğŸ”„ çŠ¶æ³ã‚’æ›´æ–°", key="dc_refresh_jobs"):
            st.rerun()

        st.markdown("---")

    # å®Œäº†ã—ãŸã‚¸ãƒ§ãƒ–ã®é€šçŸ¥ï¼ˆ5åˆ†ä»¥å†…ï¼‰
    recent_completed = {k: v for k, v in jobs.items()
                        if v.get('status') in ['completed', 'failed', 'cancelled']
                        and not v.get('is_running')}

    for job_name, progress in recent_completed.items():
        status = progress.get('status')
        message = progress.get('message', '')
        completed_at = progress.get('completed_at', '')

        if completed_at:
            try:
                completed_time = datetime.fromisoformat(completed_at)
                if (datetime.now() - completed_time).seconds < 300:
                    if status == 'completed':
                        st.success(f"âœ… {message}")
                    elif status == 'failed':
                        st.error(f"âŒ {message}")
                    elif status == 'cancelled':
                        st.warning(f"âš ï¸ {message}")
            except:
                pass


def _render_quick_collection():
    """ã‚¯ã‚¤ãƒƒã‚¯åé›†ã‚¿ãƒ–"""
    st.subheader("ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯åé›†")

    st.info("ğŸ’¡ **ä»Šæ—¥ã®ãƒ‡ãƒ¼ã‚¿**ã¯ã€Œãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è‡ªå‹•åŒ–ã€ã‚¿ãƒ–ã®ã€Œä»Šæ—¥ã®äºˆæƒ³ã‚’æº–å‚™ã€ãƒœã‚¿ãƒ³ã§ä¸€æ‹¬å–å¾—ã§ãã¾ã™")

    # å®Ÿè¡Œä¸­ã‚¸ãƒ§ãƒ–ã®çŠ¶æ…‹ç¢ºèª
    if is_job_running(JOB_DATA_COLLECTION):
        progress = get_job_progress(JOB_DATA_COLLECTION)
        st.warning("ğŸ”„ ãƒ‡ãƒ¼ã‚¿åé›†ãŒãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œä¸­ã§ã™")

        if progress:
            pct = progress.get('progress', 0)
            message = progress.get('message', 'å‡¦ç†ä¸­...')
            step = progress.get('step', '')

            if step:
                st.text(f"{step}: {message}")
            else:
                st.text(message)
            st.progress(pct / 100)

            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸ”„ çŠ¶æ³ã‚’æ›´æ–°", key="refresh_quick_collection"):
                    st.rerun()
            with col2:
                if st.button("â¹ï¸ ã‚­ãƒ£ãƒ³ã‚»ãƒ«", key="cancel_quick_collection"):
                    cancel_job(JOB_DATA_COLLECTION)
                    st.rerun()

        st.markdown("---")
        _render_collection_summary()
        return

    st.markdown("---")
    st.markdown("#### éå»ãƒ‡ãƒ¼ã‚¿åé›†")

    if st.button("ğŸ“… ä»Šé€±ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰", use_container_width=True, type="primary"):
        _start_week_data_collection_background()

    st.caption("â€» éå»1é€±é–“åˆ†ã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§åé›†ã—ã¾ã™")

    # åé›†çŠ¶æ³ã‚µãƒãƒªãƒ¼
    st.markdown("---")
    _render_collection_summary()


def _render_collection_summary():
    """åé›†çŠ¶æ³ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    st.markdown("#### ğŸ“Š ãƒ‡ãƒ¼ã‚¿åé›†çŠ¶æ³")

    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        cursor.execute("SELECT MAX(race_date) FROM races")
        latest = cursor.fetchone()[0]
        st.metric("æœ€æ–°ãƒ‡ãƒ¼ã‚¿", latest if latest else "N/A")

    with col2:
        today = datetime.now().strftime("%Y-%m-%d")
        cursor.execute("SELECT COUNT(*) FROM races WHERE race_date = ?", (today,))
        today_count = cursor.fetchone()[0]
        st.metric("æœ¬æ—¥ã®ãƒ¬ãƒ¼ã‚¹", today_count)

    with col3:
        cursor.execute("SELECT COUNT(*) FROM races")
        total = cursor.fetchone()[0]
        st.metric("ç·ãƒ¬ãƒ¼ã‚¹æ•°", f"{total:,}")

    with col4:
        cursor.execute("SELECT COUNT(*) FROM results")
        results = cursor.fetchone()[0]
        st.metric("çµæœãƒ‡ãƒ¼ã‚¿", f"{results:,}")

    conn.close()


def _collect_today_data():
    """ä»Šæ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
    st.info("ğŸ“¥ ä»Šæ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")

    try:
        from src.scraper.bulk_scraper import BulkScraper

        scraper = BulkScraper()
        schedule_scraper = scraper.schedule_scraper
        today_schedule = schedule_scraper.get_today_schedule()

        if not today_schedule:
            st.warning("æœ¬æ—¥é–‹å‚¬ã®ãƒ¬ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return

        progress_bar = st.progress(0)
        status_text = st.empty()

        total_venues = len(today_schedule)
        total_races = 0

        for i, (venue_code, race_date) in enumerate(today_schedule.items(), 1):
            status_text.text(f"ä¼šå ´ {venue_code} ã‚’åé›†ä¸­... ({i}/{total_venues})")

            result = scraper.fetch_multiple_venues(
                venue_codes=[venue_code],
                race_date=race_date,
                race_count=12
            )

            if venue_code in result:
                total_races += len(result[venue_code])

            progress_bar.progress(i / total_venues)

        st.success(f"âœ… å®Œäº†ï¼ {total_venues}ä¼šå ´ {total_races}ãƒ¬ãƒ¼ã‚¹å–å¾—ã—ã¾ã—ãŸ")

        # è£œå®Œãƒ‡ãƒ¼ã‚¿ã‚‚å–å¾—
        if st.session_state.get('quick_include_supplements', False):
            _run_all_supplement_scripts()

    except Exception as e:
        st.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")


def _collect_tomorrow_data():
    """æ˜æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
    st.info("ğŸ“¥ æ˜æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")

    try:
        from src.scraper.bulk_scraper import BulkScraper

        tomorrow = datetime.now() + timedelta(days=1)
        tomorrow_str = tomorrow.strftime("%Y-%m-%d")

        scraper = BulkScraper()

        total_races = 0
        successful_venues = []

        progress_bar = st.progress(0)
        status_text = st.empty()

        for i in range(1, 25):
            venue_code = f"{i:02d}"
            status_text.text(f"ä¼šå ´ {venue_code} ã‚’ç¢ºèªä¸­... ({i}/24)")

            try:
                result = scraper.fetch_multiple_venues(
                    venue_codes=[venue_code],
                    race_date=tomorrow_str,
                    race_count=12
                )

                if venue_code in result and result[venue_code]:
                    total_races += len(result[venue_code])
                    successful_venues.append(venue_code)

            except Exception:
                pass

            progress_bar.progress(i / 24)

        if successful_venues:
            st.success(f"âœ… å®Œäº†ï¼ {len(successful_venues)}ä¼šå ´ {total_races}ãƒ¬ãƒ¼ã‚¹å–å¾—ã—ã¾ã—ãŸ")
        else:
            st.warning("æ˜æ—¥é–‹å‚¬ã®ãƒ¬ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    except Exception as e:
        st.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")


def _start_week_data_collection_background():
    """ä»Šé€±ã®ãƒ‡ãƒ¼ã‚¿åé›†ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§é–‹å§‹"""
    script_path = os.path.join(PROJECT_ROOT, 'scripts', 'background_data_collection.py')

    result = start_job(
        JOB_DATA_COLLECTION,
        script_path,
        args=['--type', 'week']
    )

    if result['success']:
        st.success(f"âœ… {result['message']}")
        st.info("ã‚¿ãƒ–ã‚’ç§»å‹•ã—ã¦ã‚‚å‡¦ç†ã¯ç¶™ç¶šã—ã¾ã™ã€‚ãƒ˜ãƒƒãƒ€ãƒ¼ã®é€²æ—ãƒãƒ¼ã§çŠ¶æ³ã‚’ç¢ºèªã§ãã¾ã™ã€‚")
        time.sleep(1)
        st.rerun()
    else:
        st.error(f"âŒ {result['message']}")


def _render_missing_data_detector():
    """ä¸è¶³ãƒ‡ãƒ¼ã‚¿æ¤œå‡ºãƒ»å–å¾—ã‚¿ãƒ–"""
    st.subheader("ğŸ” ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã®æ¤œå‡ºã¨å–å¾—")

    # ã‚¸ãƒ§ãƒ–å®Ÿè¡Œä¸­ãƒã‚§ãƒƒã‚¯
    if is_job_running(JOB_MISSING_DATA):
        progress = get_job_progress(JOB_MISSING_DATA)
        st.warning("ğŸ”„ ä¸è¶³ãƒ‡ãƒ¼ã‚¿å–å¾—ãŒãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œä¸­ã§ã™")

        if progress:
            st.progress(progress.get('progress', 0) / 100)
            st.text(progress.get('message', 'å‡¦ç†ä¸­...'))

            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸ”„ çŠ¶æ³ã‚’æ›´æ–°", key="refresh_missing"):
                    st.rerun()
            with col2:
                if st.button("â¹ï¸ ã‚­ãƒ£ãƒ³ã‚»ãƒ«", key="cancel_missing"):
                    cancel_job(JOB_MISSING_DATA)
                    st.rerun()
        return

    # ç›´è¿‘7æ—¥é–“ã®çŠ¶æ³
    st.markdown("**ç›´è¿‘7æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ³**")
    _render_recent_data_status()

    st.markdown("---")
    st.markdown("**æœŸé–“æŒ‡å®šã§ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºãƒ»å–å¾—**")

    # æœŸé–“é¸æŠ
    col1, col2 = st.columns(2)

    with col1:
        default_start = datetime.now().date() - timedelta(days=30)
        start_date = st.date_input(
            "é–‹å§‹æ—¥",
            value=default_start,
            key="missing_start_date"
        )

    with col2:
        end_date = st.date_input(
            "çµ‚äº†æ—¥",
            value=datetime.now().date(),
            key="missing_end_date"
        )

    # DataCoverageCheckerã‚’ä½¿ç”¨ã—ã¦å…¨ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—
    try:
        checker = DataCoverageChecker(DATABASE_PATH)
        report = checker.get_coverage_report()
        all_categories = list(report["categories"].keys())
    except Exception:
        all_categories = ["ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±", "é¸æ‰‹ãƒ‡ãƒ¼ã‚¿", "ãƒ¢ãƒ¼ã‚¿ãƒ¼ãƒ»ãƒœãƒ¼ãƒˆ", "å¤©å€™ãƒ»æ°—è±¡", "æ°´é¢ãƒ»æ½®æ±", "ãƒ¬ãƒ¼ã‚¹å±•é–‹", "ã‚ªãƒƒã‚ºãƒ»äººæ°—", "çµæœãƒ‡ãƒ¼ã‚¿", "ç›´å‰æƒ…å ±", "æ‰•æˆ»ãƒ‡ãƒ¼ã‚¿"]

    # æ¤œå‡ºã‚¿ã‚¤ãƒ—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å…¨ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠï¼‰
    check_types = st.multiselect(
        "æ¤œå‡ºå¯¾è±¡ï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‰",
        all_categories,
        default=all_categories  # å…¨ã‚«ãƒ†ã‚´ãƒªã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠ
    )

    if st.button("ğŸ” ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡º", type="primary"):
        with st.spinner("ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºä¸­..."):
            missing_dates = _detect_missing_data(start_date, end_date, check_types)
            st.session_state['missing_dates'] = missing_dates
            st.session_state['missing_check_types'] = check_types

    # æ¤œå‡ºçµæœã®è¡¨ç¤º
    if 'missing_dates' in st.session_state and st.session_state['missing_dates']:
        missing_dates = st.session_state['missing_dates']

        st.markdown("---")
        st.warning(f"âš ï¸ {len(missing_dates)}ä»¶ã®ä¸è¶³ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

        with st.expander("ä¸è¶³ãƒ‡ãƒ¼ã‚¿è©³ç´°", expanded=True):
            import pandas as pd
            df = pd.DataFrame(missing_dates)
            st.dataframe(df, use_container_width=True, hide_index=True)

        col1, col2 = st.columns(2)

        with col1:
            if st.button("ğŸ“¥ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å–å¾—", type="primary"):
                _start_missing_data_job(
                    missing_dates,
                    st.session_state.get('missing_check_types', [])
                )

        with col2:
            if st.button("ğŸ“¥ ãƒ•ã‚©ã‚¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å–å¾—"):
                _fetch_missing_data_foreground(
                    missing_dates,
                    st.session_state.get('missing_check_types', [])
                )

    elif 'missing_dates' in st.session_state:
        st.success("âœ… ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ï¼")


def _render_recent_data_status():
    """ç›´è¿‘7æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ³ã‚’è¡¨ç¤ºï¼ˆDataCoverageCheckerçµ±åˆç‰ˆï¼‰"""
    try:
        checker = DataCoverageChecker(DATABASE_PATH)
        report = checker.get_coverage_report()
        missing_items = checker.get_missing_items()
    except Exception as e:
        st.warning(f"ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚«ãƒ¼ã®ã‚¨ãƒ©ãƒ¼: {e}")
        missing_items = []

    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()

    today = datetime.now().date()
    data_status = []

    for i in range(7):
        target_date = today - timedelta(days=i)
        date_str = target_date.strftime('%Y-%m-%d')

        cursor.execute("SELECT COUNT(*) FROM races WHERE race_date = ?", (date_str,))
        race_count = cursor.fetchone()[0]

        cursor.execute("""
            SELECT COUNT(*) FROM results r
            JOIN races ra ON r.race_id = ra.id
            WHERE ra.race_date = ?
        """, (date_str,))
        result_count = cursor.fetchone()[0]

        cursor.execute("""
            SELECT COUNT(*) FROM race_details rd
            JOIN races ra ON rd.race_id = ra.id
            WHERE ra.race_date = ?
        """, (date_str,))
        detail_count = cursor.fetchone()[0]

        tenji_count = 0
        try:
            cursor.execute("""
                SELECT COUNT(*) FROM original_exhibition oe
                JOIN races ra ON oe.race_id = ra.id
                WHERE ra.race_date = ?
            """, (date_str,))
            tenji_count = cursor.fetchone()[0]
        except Exception:
            pass

        if race_count == 0:
            status = "âšª æœªå–å¾—"
        elif result_count < race_count * 5:
            status = "ğŸŸ¡ çµæœä¸è¶³"
        elif detail_count < race_count * 5:
            status = "ğŸŸ¡ è©³ç´°ä¸è¶³"
        elif tenji_count == 0:
            status = "ğŸŸ  å±•ç¤ºãªã—"
        else:
            status = "ğŸŸ¢ å®Œäº†"

        data_status.append({
            'æ—¥ä»˜': date_str,
            'æ›œæ—¥': ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥'][target_date.weekday()],
            'ãƒ¬ãƒ¼ã‚¹': race_count,
            'çµæœ': result_count,
            'è©³ç´°': detail_count,
            'å±•ç¤º': tenji_count,
            'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': status
        })

    conn.close()

    import pandas as pd
    df = pd.DataFrame(data_status)
    st.dataframe(df, use_container_width=True, hide_index=True)

    # å…¨ä½“ã®ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    if missing_items:
        st.markdown("---")
        st.markdown("**âš ï¸ å…¨ä½“ã§ä¸è¶³ã—ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ï¼ˆé‡è¦åº¦é †ï¼‰**")
        top_missing = missing_items[:10]
        for item in top_missing:
            importance_stars = "â˜…" * item["importance"]
            st.text(f"{importance_stars} [{item['category']}] {item['name']} - {item['coverage']*100:.1f}% ({item['status']})")
    else:
        st.success("âœ… å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿é …ç›®ãŒå……è¶³ã—ã¦ã„ã¾ã™ï¼")


def _detect_missing_data(start_date, end_date, check_types: List[str]) -> List[Dict]:
    """
    DataCoverageCheckerã‚’ä½¿ç”¨ã—ã¦ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡º

    Args:
        start_date: é–‹å§‹æ—¥
        end_date: çµ‚äº†æ—¥
        check_types: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã‚«ãƒ†ã‚´ãƒªã®ãƒªã‚¹ãƒˆ

    Returns:
        ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
    """
    try:
        checker = DataCoverageChecker(DATABASE_PATH)
        report = checker.get_coverage_report()
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚«ãƒ¼ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return []

    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()

    missing = []
    current_date = start_date

    while current_date <= end_date:
        date_str = current_date.strftime('%Y-%m-%d')

        # æ—¥ä»˜ã®ãƒ¬ãƒ¼ã‚¹æ•°ã‚’ç¢ºèª
        cursor.execute("SELECT COUNT(*) FROM races WHERE race_date = ?", (date_str,))
        race_count = cursor.fetchone()[0]

        issues = []
        issue_details = []

        # å„ã‚«ãƒ†ã‚´ãƒªã®ä¸è¶³ã‚’ãƒã‚§ãƒƒã‚¯
        for category_name in check_types:
            if category_name not in report["categories"]:
                continue

            category_data = report["categories"][category_name]

            for item in category_data["items"]:
                # å„é …ç›®ã®å……è¶³ç‡ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ90%æœªæº€ã‚’ä¸è¶³ã¨ã™ã‚‹ï¼‰
                if item["coverage"] < 0.9:
                    # æ—¥ä»˜å˜ä½ã§ã®ãƒã‚§ãƒƒã‚¯ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
                    if race_count > 0:
                        item_name = item["name"]
                        coverage_pct = item["coverage"] * 100

                        # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ä¸è¶³ã‚’è¨˜éŒ²
                        if category_name not in [issue["category"] for issue in issue_details]:
                            issue_details.append({
                                "category": category_name,
                                "items": [f"{item_name}({coverage_pct:.0f}%)"]
                            })
                        else:
                            # æ—¢å­˜ã‚«ãƒ†ã‚´ãƒªã«é …ç›®ã‚’è¿½åŠ 
                            for detail in issue_details:
                                if detail["category"] == category_name:
                                    detail["items"].append(f"{item_name}({coverage_pct:.0f}%)")
                                    break

        # ãƒ¬ãƒ¼ã‚¹æƒ…å ±ãŒãªã„æ—¥ä»˜ã‚‚ãƒã‚§ãƒƒã‚¯
        if race_count == 0 and "ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±" in check_types:
            issues.append("ãƒ¬ãƒ¼ã‚¹æƒ…å ±ãªã—")

        # ä¸è¶³é …ç›®ã‚’æ–‡å­—åˆ—åŒ–
        if issue_details:
            for detail in issue_details:
                category = detail["category"]
                items = detail["items"][:3]  # æœ€å¤§3é …ç›®è¡¨ç¤º
                if len(detail["items"]) > 3:
                    items.append(f"ä»–{len(detail['items'])-3}é …ç›®")
                issues.append(f"{category}: " + ", ".join(items))

        if issues or race_count == 0:
            missing.append({
                'æ—¥ä»˜': date_str,
                'æ›œæ—¥': ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥'][current_date.weekday()],
                'ãƒ¬ãƒ¼ã‚¹': race_count,
                'çµæœ': 0,  # å¾Œã§è¨ˆç®—
                'è©³ç´°': 0,  # å¾Œã§è¨ˆç®—
                'å±•ç¤º': 0,  # å¾Œã§è¨ˆç®—
                'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': 'ğŸ”´ æœªå–å¾—' if race_count == 0 else 'ğŸŸ¡ çµæœä¸è¶³' if issues else 'ğŸŸ¢ å®Œäº†'
            })

        current_date += timedelta(days=1)

    conn.close()
    return missing


def _start_missing_data_job(missing_dates: List[Dict], check_types: List[str]):
    """ä¸è¶³ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§é–‹å§‹"""
    jobs_dir = os.path.join(PROJECT_ROOT, 'temp', 'jobs')
    os.makedirs(jobs_dir, exist_ok=True)

    # UIã‚«ãƒ†ã‚´ãƒªã‚’ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç”¨ã®check_typesã«å¤‰æ›
    # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¯ "ç›´å‰æƒ…å ±å–å¾—" ã¨ "å½“æ—¥ç¢ºå®šæƒ…å ±" ã®2ç¨®é¡ã®ã¿èªè­˜
    workflow_check_types = []

    # ç›´å‰æƒ…å ±å–å¾—ãŒå¿…è¦ãªã‚«ãƒ†ã‚´ãƒª
    beforeinfo_categories = {"ç›´å‰æƒ…å ±", "ãƒ¬ãƒ¼ã‚¹å±•é–‹", "ã‚ªãƒƒã‚ºãƒ»äººæ°—", "å¤©å€™ãƒ»æ°—è±¡", "æ°´é¢ãƒ»æ½®æ±"}
    # å½“æ—¥ç¢ºå®šæƒ…å ±ãŒå¿…è¦ãªã‚«ãƒ†ã‚´ãƒª
    confirmed_categories = {"ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±", "é¸æ‰‹ãƒ‡ãƒ¼ã‚¿", "ãƒ¢ãƒ¼ã‚¿ãƒ¼ãƒ»ãƒœãƒ¼ãƒˆ", "çµæœãƒ‡ãƒ¼ã‚¿", "æ‰•æˆ»ãƒ‡ãƒ¼ã‚¿"}

    if any(cat in check_types for cat in beforeinfo_categories):
        workflow_check_types.append("ç›´å‰æƒ…å ±å–å¾—")
    if any(cat in check_types for cat in confirmed_categories):
        workflow_check_types.append("å½“æ—¥ç¢ºå®šæƒ…å ±")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å½“æ—¥ç¢ºå®šæƒ…å ±ã‚’å«ã‚ã‚‹ï¼ˆçµæœãƒ‡ãƒ¼ã‚¿å–å¾—ã®ãŸã‚ï¼‰
    if not workflow_check_types:
        workflow_check_types = ["å½“æ—¥ç¢ºå®šæƒ…å ±"]

    config_path = os.path.join(jobs_dir, f'{JOB_MISSING_DATA}_config.json')
    config = {
        'missing_dates': missing_dates,
        'check_types': workflow_check_types
    }

    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)

    worker_path = os.path.join(PROJECT_ROOT, 'scripts', 'worker_missing_data.py')

    result = start_job(
        JOB_MISSING_DATA,
        worker_path,
        args=['--config', config_path]
    )

    if result['success']:
        st.success(f"âœ… {result['message']}")
        st.info("ã‚¿ãƒ–ã‚’ç§»å‹•ã—ã¦ã‚‚å‡¦ç†ã¯ç¶™ç¶šã—ã¾ã™ã€‚ã€ŒçŠ¶æ³ã‚’æ›´æ–°ã€ãƒœã‚¿ãƒ³ã§é€²æ—ã‚’ç¢ºèªã§ãã¾ã™ã€‚")

        if 'missing_dates' in st.session_state:
            del st.session_state['missing_dates']

        time.sleep(1)
        st.rerun()
    else:
        st.error(f"âŒ {result['message']}")


def _fetch_missing_data_foreground(missing_dates: List[Dict], check_types: List[str]):
    """
    ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆãƒ•ã‚©ã‚¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
    å…¨ã‚«ãƒ†ã‚´ãƒªå¯¾å¿œç‰ˆ - æ”¹å–„ç‰ˆ

    è£œå®Œã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯å…¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬å‡¦ç†ã™ã‚‹ãŸã‚ã€
    æœŸé–“å…¨ä½“ã§1å›ã ã‘å®Ÿè¡Œã™ã‚Œã°è‰¯ã„ã€‚
    """
    progress_bar = st.progress(0)
    status_text = st.empty()
    log_placeholder = st.empty()
    logs = []

    def add_log(msg):
        logs.append(f"{datetime.now().strftime('%H:%M:%S')} - {msg}")
        log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°", "\n".join(logs[-20:]), height=300)

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®è£œå®Œã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°
    CATEGORY_SCRIPTS = {
        "ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±": [],  # åŸºæœ¬æƒ…å ±ã¯ç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        "é¸æ‰‹ãƒ‡ãƒ¼ã‚¿": [],  # åŸºæœ¬æƒ…å ±å–å¾—æ™‚ã«ä¸€ç·’ã«å–å¾—ã•ã‚Œã‚‹
        "ãƒ¢ãƒ¼ã‚¿ãƒ¼ãƒ»ãƒœãƒ¼ãƒˆ": [],  # åŸºæœ¬æƒ…å ±å–å¾—æ™‚ã«ä¸€ç·’ã«å–å¾—ã•ã‚Œã‚‹
        "å¤©å€™ãƒ»æ°—è±¡": [("è£œå®Œ_å¤©å€™ãƒ‡ãƒ¼ã‚¿_æ”¹å–„ç‰ˆ.py", "å¤©å€™ãƒ‡ãƒ¼ã‚¿"), ("è£œå®Œ_é¢¨å‘ãƒ‡ãƒ¼ã‚¿_æ”¹å–„ç‰ˆ.py", "é¢¨å‘ãƒ‡ãƒ¼ã‚¿")],
        "æ°´é¢ãƒ»æ½®æ±": [],  # æ½®æ±APIãŒæœªå®Ÿè£…
        "ãƒ¬ãƒ¼ã‚¹å±•é–‹": [("è£œå®Œ_å±•ç¤ºã‚¿ã‚¤ãƒ _å…¨ä»¶_é«˜é€ŸåŒ–.py", "å±•ç¤ºã‚¿ã‚¤ãƒ ")],
        "ã‚ªãƒƒã‚ºãƒ»äººæ°—": [],  # ã‚ªãƒƒã‚ºå–å¾—ã¯åˆ¥é€”å®Ÿè£…ãŒå¿…è¦
        "çµæœãƒ‡ãƒ¼ã‚¿": [("è£œå®Œ_ãƒ¬ãƒ¼ã‚¹è©³ç´°ãƒ‡ãƒ¼ã‚¿_æ”¹å–„ç‰ˆv4.py", "ãƒ¬ãƒ¼ã‚¹è©³ç´°"), ("è£œå®Œ_æ±ºã¾ã‚Šæ‰‹ãƒ‡ãƒ¼ã‚¿_æ”¹å–„ç‰ˆ.py", "æ±ºã¾ã‚Šæ‰‹")],
        "ç›´å‰æƒ…å ±": [("è£œå®Œ_å±•ç¤ºã‚¿ã‚¤ãƒ _å…¨ä»¶_é«˜é€ŸåŒ–.py", "ç›´å‰æƒ…å ±")],
        "æ‰•æˆ»ãƒ‡ãƒ¼ã‚¿": [("è£œå®Œ_æ‰•æˆ»é‡‘ãƒ‡ãƒ¼ã‚¿.py", "æ‰•æˆ»é‡‘")]
    }

    add_log(f"=== ä¸è¶³ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ ===")
    add_log(f"å¯¾è±¡ã‚«ãƒ†ã‚´ãƒª: {', '.join(check_types)}")
    add_log(f"å¯¾è±¡æœŸé–“: {len(missing_dates)}æ—¥åˆ†")
    add_log("")

    # ãƒ•ã‚§ãƒ¼ã‚º1: ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ã®å–å¾—
    status_text.text("ãƒ•ã‚§ãƒ¼ã‚º 1/2: ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ã®å–å¾—")
    progress_bar.progress(0.1)

    if "ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±" in check_types:
        add_log("ã€ãƒ•ã‚§ãƒ¼ã‚º1ã€‘ ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ã®å–å¾—")
        missing_race_dates = [item for item in missing_dates if item.get('ãƒ¬ãƒ¼ã‚¹', 0) == 0]

        if missing_race_dates:
            add_log(f"  {len(missing_race_dates)}æ—¥åˆ†ã®ãƒ¬ãƒ¼ã‚¹æƒ…å ±ãŒä¸è¶³")
            from src.scraper.bulk_scraper import BulkScraper
            scraper = BulkScraper()

            for idx, item in enumerate(missing_race_dates):
                date_str = item['æ—¥ä»˜']
                add_log(f"  {date_str} ã®ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—ä¸­...")

                try:
                    venue_codes = [f"{i:02d}" for i in range(1, 25)]
                    result = scraper.fetch_multiple_venues(
                        venue_codes=venue_codes,
                        race_date=date_str,
                        race_count=12
                    )
                    total_races = sum(len(races) for races in result.values())
                    if total_races > 0:
                        add_log(f"  âœ… {total_races}ãƒ¬ãƒ¼ã‚¹å–å¾—")
                    else:
                        add_log(f"  âš ï¸ ãƒ¬ãƒ¼ã‚¹ãªã—ï¼ˆä¼‘å‚¬æ—¥ï¼‰")
                except Exception as e:
                    add_log(f"  âŒ ã‚¨ãƒ©ãƒ¼: {str(e)[:60]}")

                progress = 0.1 + (0.4 * (idx + 1) / len(missing_race_dates))
                progress_bar.progress(progress)
        else:
            add_log("  ã‚¹ã‚­ãƒƒãƒ—: ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ã¯å……è¶³")
    else:
        add_log("ã€ãƒ•ã‚§ãƒ¼ã‚º1ã€‘ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå¯¾è±¡å¤–ï¼‰")

    # ãƒ•ã‚§ãƒ¼ã‚º2: è£œå®Œã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ
    status_text.text("ãƒ•ã‚§ãƒ¼ã‚º 2/2: è©³ç´°ãƒ‡ãƒ¼ã‚¿ã®è£œå®Œ")
    progress_bar.progress(0.5)
    add_log("")
    add_log("ã€ãƒ•ã‚§ãƒ¼ã‚º2ã€‘ è©³ç´°ãƒ‡ãƒ¼ã‚¿ã®è£œå®Œ")

    # å®Ÿè¡Œã™ã‚‹è£œå®Œã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’åé›†ï¼ˆé‡è¤‡æ’é™¤ï¼‰
    scripts_to_run = []
    for category in check_types:
        if category in CATEGORY_SCRIPTS:
            for script_name, label in CATEGORY_SCRIPTS[category]:
                if script_name and (script_name, label, category) not in [(s[0], s[1], s[2]) for s in scripts_to_run]:
                    scripts_to_run.append((script_name, label, category))

    if scripts_to_run:
        add_log(f"  å®Ÿè¡Œã™ã‚‹è£œå®Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ: {len(scripts_to_run)}å€‹")
        add_log("")

        for idx, (script_name, label, category) in enumerate(scripts_to_run):
            script_path = os.path.join(PROJECT_ROOT, script_name)

            if os.path.exists(script_path):
                add_log(f"  [{category}] {label} è£œå®Œä¸­...")
                status_text.text(f"ãƒ•ã‚§ãƒ¼ã‚º 2/2: {label} è£œå®Œä¸­ ({idx+1}/{len(scripts_to_run)})")

                try:
                    result = subprocess.run(
                        [sys.executable, script_path],
                        capture_output=True,
                        text=True,
                        cwd=PROJECT_ROOT,
                        timeout=600,
                        encoding='utf-8',
                        errors='ignore'
                    )
                    if result.returncode == 0:
                        add_log(f"  âœ… {label} å®Œäº†")
                    else:
                        add_log(f"  âš ï¸ {label} çµ‚äº†ï¼ˆè­¦å‘Šã‚ã‚Šï¼‰")
                        if result.stderr:
                            error_lines = result.stderr.strip().split('\n')[-3:]
                            for line in error_lines:
                                add_log(f"     {line[:70]}")
                except subprocess.TimeoutExpired:
                    add_log(f"  â±ï¸ {label} ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ10åˆ†è¶…éï¼‰")
                except Exception as e:
                    add_log(f"  âŒ {label} ã‚¨ãƒ©ãƒ¼: {str(e)[:60]}")
            else:
                add_log(f"  âš ï¸ [{category}] {label} ã‚¹ã‚¯ãƒªãƒ—ãƒˆæœªå®Ÿè£… ({script_name})")

            progress = 0.5 + (0.5 * (idx + 1) / len(scripts_to_run))
            progress_bar.progress(progress)
    else:
        add_log("  å®Ÿè¡Œã™ã‚‹è£œå®Œã‚¹ã‚¯ãƒªãƒ—ãƒˆãªã—")
        progress_bar.progress(1.0)

    status_text.text("âœ… å‡¦ç†å®Œäº†ï¼")
    progress_bar.progress(1.0)
    add_log("")
    add_log("="*50)
    add_log(f"å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")

    if 'missing_dates' in st.session_state:
        del st.session_state['missing_dates']

    st.success("âœ… ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸï¼")


def _render_original_tenji():
    """ã‚ªãƒªã‚¸ãƒŠãƒ«å±•ç¤ºãƒ‡ãƒ¼ã‚¿åé›†ã‚¿ãƒ–"""
    st.subheader("ğŸ¯ ã‚ªãƒªã‚¸ãƒŠãƒ«å±•ç¤ºãƒ‡ãƒ¼ã‚¿åé›†")

    if is_job_running(JOB_TENJI):
        progress = get_job_progress(JOB_TENJI)
        st.warning("ğŸ”„ ã‚ªãƒªã‚¸ãƒŠãƒ«å±•ç¤ºåé›†ãŒãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œä¸­ã§ã™")

        if progress:
            st.progress(progress.get('progress', 0) / 100)
            st.text(progress.get('message', 'å‡¦ç†ä¸­...'))

            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸ”„ çŠ¶æ³ã‚’æ›´æ–°", key="refresh_tenji"):
                    st.rerun()
            with col2:
                if st.button("â¹ï¸ ã‚­ãƒ£ãƒ³ã‚»ãƒ«", key="cancel_tenji"):
                    cancel_job(JOB_TENJI)
                    st.rerun()
        return

    st.markdown("""
    **æ¯æ—¥å®Ÿè¡ŒãŒå¿…è¦ãªãƒ‡ãƒ¼ã‚¿:**
    - ç›´ç·šã‚¿ã‚¤ãƒ ï¼ˆchikusen_timeï¼‰
    - 1å‘¨ã‚¿ã‚¤ãƒ ï¼ˆisshu_timeï¼‰
    - å›ã‚Šè¶³ã‚¿ã‚¤ãƒ ï¼ˆmawariashi_timeï¼‰

    âš ï¸ **æ³¨æ„**: ã‚ªãƒªã‚¸ãƒŠãƒ«å±•ç¤ºãƒ‡ãƒ¼ã‚¿ã¯é™ã‚‰ã‚ŒãŸæœŸé–“ã®ã¿å…¬é–‹ã•ã‚Œã¾ã™ã€‚éå»ãƒ‡ãƒ¼ã‚¿ã¯å–å¾—ã§ãã¾ã›ã‚“ã€‚
    """)

    st.markdown("---")
    run_mode = st.radio(
        "å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰",
        ["ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼ˆæ¨å¥¨ï¼‰", "ãƒ•ã‚©ã‚¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰"],
        horizontal=True,
        help="ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰: ã‚¿ãƒ–ç§»å‹•ã—ã¦ã‚‚ç¶™ç¶š / ãƒ•ã‚©ã‚¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰: å®Œäº†ã¾ã§å¾…æ©Ÿ"
    )

    st.markdown("---")
    col1, col2 = st.columns(2)

    with col1:
        if st.button("ğŸ“… ä»Šæ—¥", key="tenji_today", type="primary", use_container_width=True):
            if "ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰" in run_mode:
                _start_tenji_job(0)
            else:
                _run_tenji_collection_foreground(0)

    with col2:
        if st.button("ğŸ“… æ˜¨æ—¥", key="tenji_yesterday", use_container_width=True):
            if "ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰" in run_mode:
                _start_tenji_job(-1)
            else:
                _run_tenji_collection_foreground(-1)

    st.caption("â€» ã‚ªãƒªã‚¸ãƒŠãƒ«å±•ç¤ºãƒ‡ãƒ¼ã‚¿ã¯ä»Šæ—¥ã¨æ˜¨æ—¥ã®ã¿å–å¾—å¯èƒ½ã§ã™")

    st.markdown("---")
    st.subheader("åé›†çŠ¶æ³")

    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()

    today = datetime.now().date()
    tenji_status = []

    for i in range(7):
        target_date = today - timedelta(days=i)
        date_str = target_date.strftime('%Y-%m-%d')

        count = 0
        try:
            # ã‚ªãƒªã‚¸ãƒŠãƒ«å±•ç¤ºãƒ‡ãƒ¼ã‚¿ã¯race_detailsãƒ†ãƒ¼ãƒ–ãƒ«ã®chikusen_timeç­‰ã«ä¿å­˜ã•ã‚Œã‚‹
            cursor.execute("""
                SELECT COUNT(*) FROM race_details rd
                JOIN races ra ON rd.race_id = ra.id
                WHERE ra.race_date = ? AND rd.chikusen_time IS NOT NULL
            """, (date_str,))
            count = cursor.fetchone()[0]
        except Exception:
            pass

        status = "ğŸŸ¢ åé›†æ¸ˆ" if count > 0 else "ğŸ”´ æœªåé›†"
        tenji_status.append({
            'æ—¥ä»˜': date_str,
            'æ›œæ—¥': ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥'][target_date.weekday()],
            'ä»¶æ•°': count,
            'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': status
        })

    conn.close()

    import pandas as pd
    df = pd.DataFrame(tenji_status)
    st.dataframe(df, use_container_width=True, hide_index=True)


def _start_tenji_job(days_offset: int):
    """ã‚ªãƒªã‚¸ãƒŠãƒ«å±•ç¤ºåé›†ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§é–‹å§‹"""
    worker_path = os.path.join(PROJECT_ROOT, 'scripts', 'worker_tenji_collection.py')

    result = start_job(
        JOB_TENJI,
        worker_path,
        args=[str(days_offset)]
    )

    if result['success']:
        st.success(f"âœ… {result['message']}")
        st.info("ã‚¿ãƒ–ã‚’ç§»å‹•ã—ã¦ã‚‚å‡¦ç†ã¯ç¶™ç¶šã—ã¾ã™ã€‚ã€ŒçŠ¶æ³ã‚’æ›´æ–°ã€ãƒœã‚¿ãƒ³ã§é€²æ—ã‚’ç¢ºèªã§ãã¾ã™ã€‚")
        time.sleep(1)
        st.rerun()
    else:
        st.error(f"âŒ {result['message']}")


def _run_tenji_collection_foreground(days_offset: int):
    """ã‚ªãƒªã‚¸ãƒŠãƒ«å±•ç¤ºåé›†ã‚’å®Ÿè¡Œï¼ˆãƒ•ã‚©ã‚¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰"""
    target_date = datetime.now().date() + timedelta(days=days_offset)

    with st.spinner(f"ã‚ªãƒªã‚¸ãƒŠãƒ«å±•ç¤ºãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­... ({target_date})"):
        try:
            script_path = os.path.join(PROJECT_ROOT, 'fetch_original_tenji_daily.py')

            if not os.path.exists(script_path):
                st.error(f"ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {script_path}")
                return

            if days_offset == 0:
                date_args = ['--today']
            else:
                target_date_str = target_date.strftime('%Y-%m-%d')
                date_args = ['--date', target_date_str]

            result = subprocess.run(
                [sys.executable, script_path] + date_args,
                capture_output=True,
                text=True,
                timeout=600,
                cwd=PROJECT_ROOT,
                encoding='utf-8'
            )

            if result.returncode == 0:
                st.success(f"âœ… {target_date} ã®ã‚ªãƒªã‚¸ãƒŠãƒ«å±•ç¤ºãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†ï¼")
                with st.expander("è©³ç´°ãƒ­ã‚°"):
                    st.code(result.stdout)
            else:
                st.error("âŒ åé›†ã«å¤±æ•—ã—ã¾ã—ãŸ")
                st.code(result.stderr)

        except subprocess.TimeoutExpired:
            st.error("âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ10åˆ†çµŒéï¼‰")
        except Exception as e:
            st.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")


# æœŸé–“æŒ‡å®šåé›†æ©Ÿèƒ½ã¯ã€Œä¸è¶³ãƒ‡ãƒ¼ã‚¿æ¤œå‡ºã€ã«çµ±åˆã•ã‚ŒãŸãŸã‚å‰Šé™¤


def _deprecated_render_period_collection():
    """æœŸé–“æŒ‡å®šåé›†ã‚¿ãƒ–"""
    st.subheader("ğŸ“… æœŸé–“æŒ‡å®šåé›†")

    # æœ€çµ‚ä¿å­˜æ—¥ã‚’å–å¾—
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    cursor.execute("SELECT MAX(race_date) FROM races")
    result = cursor.fetchone()
    conn.close()

    if result and result[0]:
        last_saved_date = datetime.strptime(result[0], '%Y-%m-%d')
        default_start = last_saved_date + timedelta(days=1)
    else:
        last_saved_date = None
        default_start = datetime.now() - timedelta(days=7)

    if last_saved_date:
        st.info(f"ğŸ“Š æœ€çµ‚ä¿å­˜æ—¥: **{last_saved_date.strftime('%Y-%m-%d')}**")
    else:
        st.warning("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    # æœŸé–“è¨­å®š
    col1, col2 = st.columns(2)

    with col1:
        start_date = st.date_input(
            "é–‹å§‹æ—¥",
            default_start.date() if isinstance(default_start, datetime) else default_start,
            key="period_start_date"
        )

    with col2:
        end_date = st.date_input(
            "çµ‚äº†æ—¥",
            datetime.now().date(),
            key="period_end_date"
        )

    if isinstance(start_date, datetime):
        start_date = start_date.date()
    if isinstance(end_date, datetime):
        end_date = end_date.date()

    target_days = (end_date - start_date).days + 1

    if target_days <= 0:
        st.success("âœ… ãƒ‡ãƒ¼ã‚¿ã¯æœ€æ–°ã§ã™ï¼å–å¾—ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # ä¼šå ´é¸æŠ
    st.markdown("#### å¯¾è±¡ä¼šå ´")
    venue_selection = st.radio(
        "ä¼šå ´é¸æŠ",
        ["ã™ã¹ã¦ã®ä¼šå ´ï¼ˆ24ä¼šå ´ï¼‰", "ç‰¹å®šã®ä¼šå ´ã‚’é¸æŠ"],
        horizontal=True,
        key="period_venue_selection",
        label_visibility="collapsed"
    )

    selected_venues = None
    if venue_selection == "ç‰¹å®šã®ä¼šå ´ã‚’é¸æŠ":
        venue_options = {f"{info['code']} - {info['name']}": info['code']
                        for venue_id, info in VENUES.items()}
        selected_names = st.multiselect(
            "ä¼šå ´ã‚’é¸æŠ",
            list(venue_options.keys()),
            default=[],
            key="period_venue_multiselect"
        )
        selected_venues = [venue_options[name] for name in selected_names]

    # å–å¾—ãƒ‡ãƒ¼ã‚¿é¸æŠ
    st.markdown("#### å–å¾—ãƒ‡ãƒ¼ã‚¿")

    col1, col2 = st.columns(2)
    with col1:
        get_basic = st.checkbox("ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ãƒ»çµæœ", value=True, key="period_basic")
    with col2:
        get_supplements = st.checkbox("è£œå®Œãƒ‡ãƒ¼ã‚¿ï¼ˆæ±ºã¾ã‚Šæ‰‹ã€è©³ç´°ã€å¤©å€™ã€é¢¨å‘ï¼‰", value=True, key="period_supplements")

    # ç¢ºèªè¡¨ç¤º
    venue_count = len(selected_venues) if selected_venues else 24
    st.warning(f"ğŸ“Š å–å¾—å¯¾è±¡: **{target_days}æ—¥åˆ†** Ã— **{venue_count}ä¼šå ´**")

    # å®Ÿè¡Œãƒœã‚¿ãƒ³
    if st.button("ğŸš€ ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹", type="primary", use_container_width=True, key="period_start"):
        _collect_period_data(
            start_date,
            end_date,
            selected_venues,
            get_basic,
            get_supplements
        )


def _collect_period_data(start_date, end_date, venue_codes, get_basic, get_supplements):
    """æœŸé–“æŒ‡å®šã§ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
    st.info(f"ğŸ“¥ {start_date} ã‹ã‚‰ {end_date} ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")

    progress_bar = st.progress(0)
    status_text = st.empty()
    log_placeholder = st.empty()
    logs = []

    def add_log(message):
        logs.append(f"{datetime.now().strftime('%H:%M:%S')} - {message}")
        log_placeholder.text_area("å®Ÿè¡Œãƒ­ã‚°", "\n".join(logs[-20:]), height=200)

    try:
        if get_basic:
            from src.scraper.bulk_scraper import BulkScraper
            scraper = BulkScraper()

            date_range = []
            current_date = start_date
            while current_date <= end_date:
                date_range.append(current_date.strftime("%Y-%m-%d"))
                current_date += timedelta(days=1)

            if not venue_codes:
                venue_codes = [f"{i:02d}" for i in range(1, 25)]

            total_tasks = len(date_range) * len(venue_codes)
            completed_tasks = 0
            total_races = 0

            add_log(f"æœŸé–“: {start_date} ï½ {end_date}")
            add_log(f"å¯¾è±¡: {len(venue_codes)}ä¼šå ´ Ã— {len(date_range)}æ—¥")

            for date_str in date_range:
                for venue_code in venue_codes:
                    status_text.text(
                        f"{date_str} - ä¼šå ´ {venue_code} ã‚’åé›†ä¸­... "
                        f"({completed_tasks}/{total_tasks})"
                    )

                    try:
                        result = scraper.fetch_multiple_venues(
                            venue_codes=[venue_code],
                            race_date=date_str,
                            race_count=12
                        )

                        if venue_code in result:
                            total_races += len(result[venue_code])

                    except Exception:
                        pass

                    completed_tasks += 1
                    progress_bar.progress(completed_tasks / total_tasks)

            add_log(f"âœ… ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±: {total_races}ãƒ¬ãƒ¼ã‚¹å–å¾—å®Œäº†")

        if get_supplements:
            add_log("è£œå®Œãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚’é–‹å§‹...")
            _run_all_supplement_scripts(add_log, status_text)

        status_text.text("âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        add_log("ğŸ‰ ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†ï¼")

        _show_period_data_summary(start_date, end_date)

        st.success("âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

    except Exception as e:
        st.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        add_log(f"âŒ è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {str(e)}")


def _run_all_supplement_scripts(add_log=None, status_text=None):
    """ã™ã¹ã¦ã®è£œå®Œã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ"""
    tasks = [
        ("æ±ºã¾ã‚Šæ‰‹ãƒ‡ãƒ¼ã‚¿", "è£œå®Œ_æ±ºã¾ã‚Šæ‰‹ãƒ‡ãƒ¼ã‚¿_æ”¹å–„ç‰ˆ.py"),
        ("ãƒ¬ãƒ¼ã‚¹è©³ç´°ãƒ‡ãƒ¼ã‚¿v4", "è£œå®Œ_ãƒ¬ãƒ¼ã‚¹è©³ç´°ãƒ‡ãƒ¼ã‚¿_æ”¹å–„ç‰ˆv4.py"),
        ("å¤©å€™ãƒ‡ãƒ¼ã‚¿", "è£œå®Œ_å¤©å€™ãƒ‡ãƒ¼ã‚¿_æ”¹å–„ç‰ˆ.py"),
        ("é¢¨å‘ãƒ‡ãƒ¼ã‚¿", "è£œå®Œ_é¢¨å‘ãƒ‡ãƒ¼ã‚¿_æ”¹å–„ç‰ˆ.py"),
    ]

    if add_log is None:
        add_log = lambda x: None
    if status_text is None:
        status_text = st.empty()

    for task_name, script_name in tasks:
        status_text.text(f"{task_name}ã‚’å‡¦ç†ä¸­...")
        add_log(f"{task_name}ã®å‡¦ç†ã‚’é–‹å§‹")

        try:
            script_path = os.path.join(PROJECT_ROOT, script_name)

            if not os.path.exists(script_path):
                add_log(f"âš ï¸ {task_name}: ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue

            result = subprocess.run(
                [sys.executable, script_path],
                capture_output=True,
                text=True,
                cwd=PROJECT_ROOT,
                timeout=600,
                encoding='utf-8'
            )

            if result.returncode == 0:
                add_log(f"âœ… {task_name}: å®Œäº†")
            else:
                add_log(f"âš ï¸ {task_name}: è­¦å‘Šã‚ã‚Š")

        except subprocess.TimeoutExpired:
            add_log(f"â±ï¸ {task_name}: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
        except Exception as e:
            add_log(f"âŒ {task_name}: ã‚¨ãƒ©ãƒ¼ - {str(e)[:100]}")


def _show_period_data_summary(start_date, end_date):
    """æœŸé–“ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    st.subheader("ğŸ“Š å–å¾—ãƒ‡ãƒ¼ã‚¿ç¢ºèª")

    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        start_str = start_date.strftime('%Y-%m-%d') if hasattr(start_date, 'strftime') else str(start_date)
        end_str = end_date.strftime('%Y-%m-%d') if hasattr(end_date, 'strftime') else str(end_date)

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            cursor.execute("""
                SELECT COUNT(*) FROM races
                WHERE race_date BETWEEN ? AND ?
            """, (start_str, end_str))
            race_count = cursor.fetchone()[0]
            st.metric("ãƒ¬ãƒ¼ã‚¹æ•°", f"{race_count:,}")

        with col2:
            cursor.execute("""
                SELECT COUNT(*) FROM results r
                JOIN races ra ON r.race_id = ra.id
                WHERE ra.race_date BETWEEN ? AND ?
            """, (start_str, end_str))
            result_count = cursor.fetchone()[0]
            st.metric("çµæœãƒ‡ãƒ¼ã‚¿", f"{result_count:,}")

        with col3:
            cursor.execute("""
                SELECT COUNT(*) FROM race_details rd
                JOIN races ra ON rd.race_id = ra.id
                WHERE ra.race_date BETWEEN ? AND ?
            """, (start_str, end_str))
            detail_count = cursor.fetchone()[0]
            st.metric("ãƒ¬ãƒ¼ã‚¹è©³ç´°", f"{detail_count:,}")

        with col4:
            cursor.execute("""
                SELECT COUNT(*) FROM results r
                JOIN races ra ON r.race_id = ra.id
                WHERE ra.race_date BETWEEN ? AND ? AND r.kimarite IS NOT NULL
            """, (start_str, end_str))
            kimarite_count = cursor.fetchone()[0]
            if result_count > 0:
                ratio = kimarite_count / result_count * 100
                st.metric("æ±ºã¾ã‚Šæ‰‹", f"{ratio:.1f}%")
            else:
                st.metric("æ±ºã¾ã‚Šæ‰‹", "0%")

        conn.close()

    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
