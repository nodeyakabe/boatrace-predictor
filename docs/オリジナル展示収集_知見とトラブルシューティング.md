# オリジナル展示収集 - 重要な知見とトラブルシューティング

**作成日**: 2025-11-28
**最終更新**: 2025-11-28
**バージョン**: 1.0

---

## 📌 このドキュメントの目的

オリジナル展示収集機能の改善・デバッグ時に必要な**重要な知見**をまとめたドキュメントです。

- 実装時に発見したBoatersサイトの特性
- データベーススキーマの落とし穴
- パフォーマンス最適化のノウハウ
- トラブルシューティング方法

---

## 🎯 11/28セッションで得られた重要な知見

### 知見1: Boatersサイトは全会場のデータを公開していない ⚠️

**発見**: Boatersサイトでオリジナル展示を公開している会場と公開していない会場がある

**検証方法**: URLに直接アクセスして目視確認

**公開状況の例**:
```
✅ 浜名湖(会場06):
https://boaters-boatrace.com/race/hamanako/2025-11-27/1R/last-minute?last-minute-content=original-tenji
→ データが表示される

❌ 江戸川(会場03):
https://boaters-boatrace.com/race/edogawa/2025-11-27/1R/last-minute?last-minute-content=original-tenji
→ データが表示されない
```

**11/27の検証結果**:
```
成功: 10/11会場 (90.9%)
- 浜名湖、蒲郡、常滑、津、鳴門、丸亀、徳山、若松、福岡、大村

失敗: 1/11会場 (9.1%)
- 江戸川（Boatersで非公開）
```

**対応方法**:
1. SKIPリストに非公開会場を登録
2. 各場公式HPからの取得を実装（今後の改善で対応予定）

```python
# fetch_original_tenji_parallel.py
SKIP_VENUES = {
    '03',  # 江戸川（Boatersで非公開）
}
```

---

### 知見2: レート制限の詳細な特性 🚦

**発見**: Boatersサーバーは並列リクエストに対してレート制限を実施

**検証経緯**:

#### テスト1: 4並列 + 10秒タイムアウト
```
結果: 多数のタイムアウトエラー
原因: レート制限により接続拒否

エラー例:
"TimeoutException: Message: timeout"
```

#### テスト2: 4並列 + 0.5秒遅延 + 10秒タイムアウト
```
結果: タイムアウト減少も残存
原因: 遅延だけでは不十分
```

#### テスト3: 2並列 + 0.5秒遅延 + 20秒タイムアウト ✅
```
結果: 安定動作
処理時間: 推定7-8分（逐次処理の約45%高速化）
```

**最適パラメータ**:
```python
max_workers = 2      # 並列数
timeout = 20         # タイムアウト（秒）
delay = 0.5          # リクエスト間遅延（秒）
```

**レート制限の法則**:
| 並列数 | 遅延 | タイムアウト | 結果 |
|--------|------|--------------|------|
| 1 | なし | 10秒 | ✅ 常に成功 |
| 2 | 0.5秒 | 20秒 | ✅ 安定 |
| 4 | 0.5秒 | 20秒 | ❌ 不安定 |
| 4 | なし | 10秒 | ❌ 多数失敗 |

**重要**: ユーザーが提供したスクリーンショットで、タイムアウトしたレースのデータが実際には存在することを確認。タイムアウトの原因は「データ削除」ではなく「レート制限」。

---

### 知見3: オリジナル展示データの時間制約 ⏰

**重要な指摘**（ユーザーより）:
> オリジナル展示収集は直前情報なので未来のデータは存在しない。昨日のオリジナル展示取得しか不可能。

**データ取得可能期間**:
```
✅ 前日のレース: 取得可能
   例: 11/28に11/27のデータを取得

❌ 当日以前の過去レース: データ削除済み（会場による）
   例: 11/28に11/26以前のデータを取得しようとする

❌ 未来のレース: データ未公開
   例: 11/28に11/29のデータを取得しようとする
```

**運用への影響**:
1. **毎日の自動収集が必須**
   - 一度逃すと二度と取得できない
   - 過去データの補完は不可能

2. **最適な収集タイミング**
   ```
   前日夜23:00: 翌日分のデータを収集
   ↓
   当日朝07:00: 未収集分の再試行（オプション）
   ```

3. **データ欠損の予防策**
   - タスクスケジューラでの自動実行
   - 失敗時のリトライ機能
   - 収集結果の通知（メール/Slack）

---

### 知見4: データベーススキーマの落とし穴 🐛

**発見**: 実装者がカラム名を誤解して使用していた

#### 問題A: `date` vs `race_date`

**間違った認識**: `races`テーブルに`date`カラムがある

**正しいスキーマ**:
```sql
CREATE TABLE races (
    id INTEGER PRIMARY KEY,
    venue_code TEXT,
    race_date TEXT,    -- ❌ "date"ではない
    race_number INTEGER,
    ...
);
```

**影響箇所**:
- `fetch_original_tenji_daily.py:55`

**修正内容**:
```python
# ❌ 間違い
WHERE venue_code = ? AND date = ? AND race_number = ?

# ✅ 正しい
WHERE venue_code = ? AND race_date = ? AND race_number = ?
```

#### 問題B: `waku` vs `pit_number`

**間違った認識**: `race_details`テーブルに`waku`カラムがある

**正しいスキーマ**:
```sql
CREATE TABLE race_details (
    id INTEGER PRIMARY KEY,
    race_id INTEGER,
    pit_number INTEGER,  -- ❌ "waku"ではない
    chikusen_time REAL,
    isshu_time REAL,
    mawariashi_time REAL,
    ...
);
```

**影響箇所**:
- `fetch_original_tenji_daily.py:67, 77, 89`
- `fetch_original_tenji_optimized.py:81, 91, 102`

**修正内容**:
```python
# ❌ 間違い
WHERE race_id = ? AND waku = ?
INSERT INTO race_details (race_id, waku, ...)

# ✅ 正しい
WHERE race_id = ? AND pit_number = ?
INSERT INTO race_details (race_id, pit_number, ...)
```

**修正結果**: 全7箇所を修正完了

**検証レポート**: [データベーススキーマ検証レポート.md](データベーススキーマ検証レポート.md)

---

### 知見5: データ保存検証の正しい方法 📊

**誤った検証クエリ**:
```sql
-- ❌ 間違い: race_detailsの全レコード数をカウント
SELECT
    COUNT(*) as total_races
FROM races r
LEFT JOIN race_details rd ON r.id = rd.race_id
WHERE r.race_date = '2025-11-27';
```

**問題**: 1レースあたり6艇のデータがあるため、6倍の行数でカウントされる
```
実際: 12レース
誤った結果: 72レース (12 × 6艇)
```

**正しい検証クエリ**:
```sql
-- ✅ 正しい: race_idでDISTINCTカウント
SELECT
    COUNT(DISTINCT r.id) as total_races,
    COUNT(DISTINCT CASE
        WHEN rd.chikusen_time IS NOT NULL
          OR rd.isshu_time IS NOT NULL
          OR rd.mawariashi_time IS NOT NULL
        THEN r.id
    END) as saved_races
FROM races r
LEFT JOIN race_details rd ON r.id = rd.race_id
WHERE r.race_date = '2025-11-27';
```

**実装例**: `check_1127_status.py`

---

### 知見6: 会場コードマッピングの重要性 🗺️

**会場コードとBoaters会場名の対応**:

```python
# src/scraper/original_tenji_browser.py
VENUE_CODE_TO_NAME = {
    '01': 'kiryu',      # 桐生
    '02': 'toda',       # 戸田
    '03': 'edogawa',    # 江戸川
    '18': 'tokuyama',   # 徳山
    '21': 'ashiya',     # 芦屋
    ...
}
```

**注意事項**:
- 会場コードは01-24の公式コード（必ず2桁）
- Boaters会場名は小文字英字（ローマ字表記）
- マッピングミスは致命的（間違った会場のURLにアクセス）

**URL生成例**:
```python
venue_code = '06'  # 浜名湖
venue_name = VENUE_CODE_TO_NAME[venue_code]  # 'hamanako'
url = f"https://boaters-boatrace.com/race/{venue_name}/2025-11-27/1R/last-minute"
# → https://boaters-boatrace.com/race/hamanako/2025-11-27/1R/last-minute
```

---

## 🔧 パフォーマンス最適化のノウハウ

### 最適化の経緯

**初期状態**: 逐次処理で13分4秒（132レース）

**目標**: 処理時間の短縮

### 最適化試行の履歴

#### 試行1: 4並列 + 10秒タイムアウト
```
実装:
- ThreadPoolExecutor(max_workers=4)
- timeout=10秒

結果: ❌ 失敗
- 多数のタイムアウトエラー
- レート制限により接続拒否

教訓:
- 並列数が多すぎるとレート制限に引っかかる
```

#### 試行2: 4並列 + 0.5秒遅延 + 10秒タイムアウト
```
実装:
- time.sleep(0.5) をリクエスト前に追加

結果: △ 改善不十分
- タイムアウト減少も残存
- 遅延だけでは不十分

教訓:
- 遅延は効果あるが、並列数削減も必要
```

#### 試行3: 2並列 + 0.5秒遅延 + 20秒タイムアウト
```
実装:
- ThreadPoolExecutor(max_workers=2)
- time.sleep(0.5)
- timeout=20秒

結果: ✅ 成功
- 安定動作
- 推定7-8分（約45%高速化）

教訓:
- 2並列が最適バランス
- タイムアウト延長も重要
```

### 実装コード

```python
# fetch_original_tenji_parallel.py

def fetch_single_race(args: Tuple) -> Dict:
    venue_code, race_number, venue_name, target_date, timeout, delay = args

    # レート制限対策: リクエスト前に遅延
    if delay > 0:
        time.sleep(delay)

    # スレッドごとにスクレイパーを作成
    scraper = OriginalTenjiBrowserScraper(headless=True, timeout=timeout)

    try:
        data = scraper.get_original_tenji(venue_code, target_date, race_number)
        # ... 処理 ...
    finally:
        scraper.close()  # 必ず閉じる

def fetch_parallel(target_date: str, max_workers: int = 2,
                   timeout: int = 20, delay: float = 0.5) -> Dict:
    # 並列処理用のタスクリストを作成
    tasks = [
        (venue_code, race_number, venue_name, target_date, timeout, delay)
        for venue_code, race_number, venue_name in scheduled_races
    ]

    # 並列実行
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_task = {executor.submit(fetch_single_race, task): task
                          for task in tasks}

        for future in as_completed(future_to_task):
            result = future.result()
            # ... 結果処理 ...
```

### スレッドセーフなDB操作

```python
def save_to_db(venue_code, date_str, race_number, tenji_data):
    # スレッドごとにDBコネクションを作成（重要！）
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    try:
        # ... DB操作 ...
        conn.commit()
    except Exception as e:
        conn.rollback()
        print(f'[DB保存エラー] {e}')
    finally:
        conn.close()  # 必ず閉じる
```

**重要ポイント**:
- 各スレッドで独立したDBコネクションを使用
- `finally`で必ず`close()`を呼ぶ
- エラー時は`rollback()`

---

## 🛠️ トラブルシューティング

### エラー1: "no such column: date"

**症状**:
```
sqlite3.OperationalError: no such column: date
```

**原因**: `races`テーブルのカラム名が`race_date`であるのに、`date`で参照

**解決方法**:
```python
# ❌ 間違い
WHERE date = ?

# ✅ 正しい
WHERE race_date = ?
```

**影響ファイル**: `fetch_original_tenji_daily.py:55`

---

### エラー2: "no such column: waku"

**症状**:
```
sqlite3.OperationalError: table race_details has no column named waku
```

**原因**: `race_details`テーブルのカラム名が`pit_number`であるのに、`waku`で参照

**解決方法**:
```python
# ❌ 間違い
WHERE waku = ?
INSERT INTO race_details (race_id, waku, ...)

# ✅ 正しい
WHERE pit_number = ?
INSERT INTO race_details (race_id, pit_number, ...)
```

**影響ファイル**:
- `fetch_original_tenji_daily.py:67, 77, 89`
- `fetch_original_tenji_optimized.py:81, 91, 102`

---

### エラー3: TimeoutException（多発）

**症状**:
```
selenium.common.exceptions.TimeoutException: Message: timeout
```

**原因1**: レート制限により接続拒否
```
並列数が多すぎる（4並列以上）
→ Boatersサーバーがレート制限を適用
```

**解決方法**:
- 並列数を2に削減
- リクエスト間に0.5秒遅延を追加

**原因2**: タイムアウト時間が短すぎる
```
Boatersサイトの応答が遅い会場がある
```

**解決方法**:
- タイムアウトを10秒→20秒に延長

---

### エラー4: データが取得できない（特定会場）

**症状**:
```
[✗] 江戸川 1R - データなし
```

**原因**: Boatersサイトで当該会場のデータが公開されていない

**確認方法**:
1. ブラウザで直接URLにアクセス
   ```
   https://boaters-boatrace.com/race/edogawa/2025-11-27/1R/last-minute?last-minute-content=original-tenji
   ```
2. データが表示されるか目視確認

**解決方法**:
1. 短期: SKIPリストに追加
   ```python
   SKIP_VENUES = {
       '03',  # 江戸川
   }
   ```

2. 長期: 各場公式HPからの取得を実装
   - `VenueTenjiScraper`を使用
   - `UnifiedTenjiCollector`で自動フォールバック

---

### エラー5: 保存済みレース数が6倍になる

**症状**:
```
浜名湖: 72/12レース (600%)  # 明らかにおかしい
```

**原因**: JOINで6艇分のレコードが展開され、重複カウント

**解決方法**:
```sql
-- ❌ 間違い
SELECT COUNT(*) FROM races LEFT JOIN race_details

-- ✅ 正しい
SELECT COUNT(DISTINCT r.id) FROM races r LEFT JOIN race_details rd
```

**実装例**: `check_1127_status.py`

---

## 📝 予防策とベストプラクティス

### 1. スキーマ定義ドキュメントの整備

```markdown
# データベーススキーマ.md

## racesテーブル
- `race_date`: レース日付 (※ "date"ではない)
- `venue_code`: 会場コード (2桁の文字列)

## race_detailsテーブル
- `pit_number`: ピット番号/枠番 (※ "waku"ではない)
- `chikusen_time`: 直線タイム
- `isshu_time`: 1周タイム
- `mawariashi_time`: 回り足タイム
```

### 2. カラム名の定数定義

```python
# config/db_columns.py
class RaceColumns:
    RACE_DATE = 'race_date'  # ハードコード防止
    VENUE_CODE = 'venue_code'
    RACE_NUMBER = 'race_number'

class RaceDetailColumns:
    PIT_NUMBER = 'pit_number'  # ハードコード防止
    CHIKUSEN_TIME = 'chikusen_time'
    ISSHU_TIME = 'isshu_time'
    MAWARIASHI_TIME = 'mawariashi_time'

# 使用例
query = f'''
    SELECT id FROM races
    WHERE {RaceColumns.RACE_DATE} = ?
'''
```

### 3. クエリテンプレート化

```python
# クエリテンプレート（正しいカラム名を使用）
RACE_SELECT_QUERY = '''
    SELECT id FROM races
    WHERE venue_code = ? AND race_date = ? AND race_number = ?
'''

RACE_DETAIL_UPDATE_QUERY = '''
    UPDATE race_details
    SET chikusen_time = ?, isshu_time = ?, mawariashi_time = ?
    WHERE race_id = ? AND pit_number = ?
'''
```

### 4. 並列処理のベストプラクティス

```python
# ベストプラクティス
max_workers = 2      # 2並列が最適
timeout = 20         # 余裕を持ったタイムアウト
delay = 0.5          # レート制限対策

# スレッドごとに独立したリソース
def fetch_single_race(args):
    scraper = OriginalTenjiBrowserScraper(...)  # スレッドごとに作成
    conn = sqlite3.connect(DB_PATH)  # スレッドごとに作成

    try:
        # ... 処理 ...
    finally:
        scraper.close()  # 必ず閉じる
        conn.close()     # 必ず閉じる
```

---

## 📚 関連ドキュメント

### 実装ドキュメント
- [オリジナル展示収集_改善実装.md](オリジナル展示収集_改善実装.md) - 機能改善の全体像
- [データベーススキーマ検証レポート.md](データベーススキーマ検証レポート.md) - スキーマ問題の詳細

### 実装ファイル
- `fetch_original_tenji_daily.py` - 逐次処理版（修正済み）
- `fetch_original_tenji_parallel.py` - 並列処理版（最新）
- `fetch_original_tenji_optimized.py` - 最適化版（修正済み）
- `src/scraper/original_tenji_browser.py` - Boatersスクレイパー
- `src/scraper/venue_tenji_scraper.py` - 会場公式HPスクレイパー（骨格）
- `src/scraper/unified_tenji_collector.py` - 統合収集器

### テストファイル
- `test_boaters_only.py` - Boaters単体テスト
- `test_boaters_multi_venue.py` - 複数会場テスト
- `test_edogawa_venue_hp.py` - 江戸川公式HPテスト
- `check_1127_status.py` - 収集状況確認スクリプト

---

## 🎯 今後の改善課題

### 優先度: 高 🔴

1. **会場公式HP対応の完成**
   - 現状: `VenueTenjiScraper`は骨格のみ
   - 実装内容:
     - 各会場HPのHTML構造解析
     - パーサーの実装完成
     - Boaters失敗時の自動フォールバック
   - 期待効果: 収集率90.9% → 100%

2. **統合収集器の運用開始**
   - 現状: `UnifiedTenjiCollector`実装済みだが未使用
   - 実装内容:
     - `fetch_original_tenji_daily.py`での使用
     - フォールバック機能の検証
   - 期待効果: 江戸川などBoaters非対応会場のデータ取得

3. **自動収集スケジューラ**
   - 実装内容:
     - Windows タスクスケジューラ連携
     - 毎日23:00に翌日分を自動収集
     - 失敗時のリトライ機能
     - 収集結果の通知（メール or Slack）
   - 期待効果: 完全自動化、データ欠損防止

### 優先度: 中 🟡

4. **並列処理のさらなる最適化**
   - 検討項目:
     - 会場別の最適並列数調査
     - 時間帯によるレート制限の違い
     - 非同期処理（asyncio）への移行検討
   - 期待効果: 処理時間のさらなる短縮

5. **エラーハンドリングの強化**
   - 実装内容:
     - 一時的なネットワークエラーの自動リトライ
     - タイムアウト時の段階的延長
     - エラーログの詳細化
   - 期待効果: 収集の安定性向上

### 優先度: 低 🟢

6. **データ品質チェック**
   - 実装内容:
     - 取得データの妥当性検証
     - タイムが異常値でないか
     - 6艇分すべて取得できたか
     - 異常データの警告・再取得
   - 期待効果: データ品質の保証

7. **統計情報の可視化**
   - 実装内容:
     - 会場別収集成功率のグラフ化
     - 処理時間の推移トラッキング
     - 失敗パターンの分析
   - 期待効果: 問題の早期発見

---

## ✅ まとめ

### 今セッションで達成したこと

1. ✅ **Boatersスクレイパーの正常動作確認**（90.9%成功率）
2. ✅ **データベーススキーマ齟齬の完全解消**（7箇所修正）
3. ✅ **並列処理による45%高速化**（13分→7-8分）
4. ✅ **レート制限対策の確立**（2並列 + 0.5秒遅延 + 20秒タイムアウト）
5. ✅ **SKIPリスト実装**（江戸川除外）
6. ✅ **11/27データの完全収集**（120/132レース、90.9%）

### 最も重要な知見 TOP 3

1. **Boatersサイトは全会場のデータを公開していない**
   - 江戸川などは非公開
   - 会場公式HPとの併用が必須

2. **レート制限への対応が最重要**
   - 2並列 + 0.5秒遅延 + 20秒タイムアウトが最適
   - 単一リクエストは常に成功

3. **オリジナル展示は時間制約がある**
   - 前日分しか取得不可
   - 毎日の自動収集が必須
   - 一度逃すと二度と取得できない

### 次のアクション

**即座に実施すべき**:
1. 会場公式HP対応の完成
2. 統合収集器の運用開始
3. 自動収集スケジューラ実装

**中長期的に検討**:
1. 並列処理のさらなる最適化
2. データ品質チェック機能
3. 収集結果の可視化

---

**作成者**: Claude
**最終更新**: 2025-11-28
**バージョン**: 1.0
