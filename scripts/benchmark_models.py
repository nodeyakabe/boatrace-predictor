"""
ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
æ–°æ—§ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒè©•ä¾¡
"""
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import pickle
import json
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score


def load_model(model_path):
    """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
    if not os.path.exists(model_path):
        return None
    try:
        with open(model_path, 'rb') as f:
            return pickle.load(f)
    except:
        return None


def prepare_test_data(venue_code=None, days=30):
    """
    ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆæœ€è¿‘ã®ãƒ‡ãƒ¼ã‚¿ï¼‰

    Args:
        venue_code: ä¼šå ´ã‚³ãƒ¼ãƒ‰
        days: éå»ä½•æ—¥åˆ†

    Returns:
        X, y, race_info
    """
    from config.settings import DATABASE_PATH
    from src.features.optimized_features import OptimizedFeatureGenerator
    from src.features.timeseries_features import TimeseriesFeatureGenerator

    conn = sqlite3.connect(DATABASE_PATH)

    # æ—¥ä»˜ç¯„å›²
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)

    # ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—
    query = """
        SELECT
            r.id as race_id,
            r.venue_code,
            r.race_date,
            r.race_number,
            e.pit_number,
            e.racer_number,
            e.racer_name,
            e.motor_number,
            res.rank,
            r.race_grade
        FROM results res
        JOIN races r ON res.race_id = r.id
        JOIN entries e ON res.race_id = e.race_id AND res.pit_number = e.pit_number
        WHERE r.race_date BETWEEN ? AND ?
    """

    params = [start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')]

    if venue_code:
        query += " AND r.venue_code = ?"
        params.append(venue_code)

    query += " ORDER BY r.race_date DESC, r.venue_code, r.race_number, e.pit_number"

    df = pd.read_sql_query(query, conn, params=params)

    if len(df) == 0:
        conn.close()
        return None, None, None

    # ç‰¹å¾´é‡ç”Ÿæˆ
    opt_feature_gen = OptimizedFeatureGenerator(DATABASE_PATH)
    ts_feature_gen = TimeseriesFeatureGenerator(conn)

    features_list = []
    labels_list = []
    race_info_list = []

    for idx, row in df.iterrows():
        try:
            # ç‰¹å¾´é‡ç”Ÿæˆ
            opt_features = opt_feature_gen.generate_all_features(
                row['racer_number'],
                row['venue_code'],
                row['race_date']
            )

            ts_features = ts_feature_gen.generate_all_timeseries_features(
                row['racer_number'],
                row['motor_number'],
                row['venue_code'],
                row['race_date']
            )

            combined_features = {**opt_features, **ts_features}
            combined_features['pit_number'] = row['pit_number']

            label = 1 if row['rank'] == 1 else 0

            features_list.append(combined_features)
            labels_list.append(label)
            race_info_list.append({
                'race_id': row['race_id'],
                'venue_code': row['venue_code'],
                'race_date': row['race_date'],
                'race_number': row['race_number'],
                'pit_number': row['pit_number'],
                'racer_name': row['racer_name'],
                'actual_rank': row['rank']
            })

        except:
            pass

    conn.close()

    X = pd.DataFrame(features_list).fillna(0).replace([np.inf, -np.inf], 0)
    y = np.array(labels_list)

    return X, y, race_info_list


def evaluate_model(model, X, y, model_name):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡

    Args:
        model: è©•ä¾¡å¯¾è±¡ãƒ¢ãƒ‡ãƒ«
        X: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        y: ãƒ©ãƒ™ãƒ«
        model_name: ãƒ¢ãƒ‡ãƒ«å

    Returns:
        dict: è©•ä¾¡æŒ‡æ¨™
    """
    if model is None or X is None or len(X) == 0:
        return None

    # äºˆæ¸¬
    y_pred_proba = model.predict_proba(X)[:, 1]
    y_pred = (y_pred_proba >= 0.5).astype(int)

    # è©•ä¾¡æŒ‡æ¨™
    metrics = {
        'model_name': model_name,
        'n_samples': len(y),
        'auc': float(roc_auc_score(y, y_pred_proba)),
        'accuracy': float(accuracy_score(y, y_pred)),
        'precision': float(precision_score(y, y_pred, zero_division=0)),
        'recall': float(recall_score(y, y_pred, zero_division=0)),
        'f1': float(f1_score(y, y_pred, zero_division=0))
    }

    return metrics


def compare_models(old_model_path, new_model_path, venue_code=None):
    """
    æ–°æ—§ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒ

    Args:
        old_model_path: æ—§ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
        new_model_path: æ–°ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
        venue_code: ä¼šå ´ã‚³ãƒ¼ãƒ‰

    Returns:
        dict: æ¯”è¼ƒçµæœ
    """
    print(f"\n{'='*60}")
    print(f"ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ: {'å…¨ä¼šå ´' if venue_code is None else f'ä¼šå ´{venue_code}'}")
    print(f"{'='*60}")

    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    old_model = load_model(old_model_path)
    new_model = load_model(new_model_path)

    if old_model is None and new_model is None:
        print("âš ï¸ ä¸¡æ–¹ã®ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        return None

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
    print("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
    X, y, race_info = prepare_test_data(venue_code=venue_code, days=30)

    if X is None or len(X) == 0:
        print("âš ï¸ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        return None

    print(f"ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X):,}")

    # æ—§ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    old_metrics = None
    if old_model is not None:
        print("\næ—§ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")
        old_metrics = evaluate_model(old_model, X, y, "æ—§ãƒ¢ãƒ‡ãƒ«")
        if old_metrics:
            print(f"  AUC: {old_metrics['auc']:.4f}")
            print(f"  Accuracy: {old_metrics['accuracy']:.4f}")

    # æ–°ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    new_metrics = None
    if new_model is not None:
        print("\næ–°ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")
        new_metrics = evaluate_model(new_model, X, y, "æ–°ãƒ¢ãƒ‡ãƒ«")
        if new_metrics:
            print(f"  AUC: {new_metrics['auc']:.4f}")
            print(f"  Accuracy: {new_metrics['accuracy']:.4f}")

    # æ¯”è¼ƒ
    comparison = {
        'venue_code': venue_code if venue_code else 'general',
        'old_model': old_metrics,
        'new_model': new_metrics,
        'n_test_samples': len(X)
    }

    if old_metrics and new_metrics:
        comparison['improvements'] = {
            'auc_diff': new_metrics['auc'] - old_metrics['auc'],
            'auc_improvement_pct': ((new_metrics['auc'] - old_metrics['auc']) / old_metrics['auc']) * 100,
            'accuracy_diff': new_metrics['accuracy'] - old_metrics['accuracy']
        }

        print("\næ”¹å–„åº¦:")
        print(f"  AUCæ”¹å–„: {comparison['improvements']['auc_diff']:+.4f} ({comparison['improvements']['auc_improvement_pct']:+.2f}%)")
        print(f"  Accuracyæ”¹å–„: {comparison['improvements']['accuracy_diff']:+.4f}")

    return comparison


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n" + "="*60)
    print(" ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print(" æ–°æ—§ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒè©•ä¾¡")
    print("="*60 + "\n")

    comparisons = []

    # æ±ç”¨ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ
    print("\nã€æ±ç”¨ãƒ¢ãƒ‡ãƒ«ã€‘")
    general_comparison = compare_models(
        'models/stage2_combined_8months.pkl',
        'models/optimized_general.pkl',
        venue_code=None
    )
    if general_comparison:
        comparisons.append(general_comparison)

    # ä¼šå ´åˆ¥ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ
    top_venues = ['07', '08', '05', '14', '09']

    for venue_code in top_venues:
        print(f"\nã€ä¼šå ´{venue_code}ã€‘")
        venue_comparison = compare_models(
            f'models/stage2_venue_{venue_code}.pkl',
            f'models/optimized_venue_{venue_code}.pkl',
            venue_code=venue_code
        )
        if venue_comparison:
            comparisons.append(venue_comparison)

    # ç·åˆãƒ¬ãƒãƒ¼ãƒˆ
    print("\n" + "="*60)
    print(" ç·åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆ")
    print("="*60 + "\n")

    for comp in comparisons:
        venue = comp['venue_code']
        print(f"ã€{venue}ã€‘")

        if comp.get('old_model'):
            print(f"  æ—§ãƒ¢ãƒ‡ãƒ«AUC: {comp['old_model']['auc']:.4f}")
        if comp.get('new_model'):
            print(f"  æ–°ãƒ¢ãƒ‡ãƒ«AUC: {comp['new_model']['auc']:.4f}")

        if comp.get('improvements'):
            imp = comp['improvements']
            symbol = "ğŸ“ˆ" if imp['auc_diff'] > 0 else "ğŸ“‰"
            print(f"  {symbol} æ”¹å–„åº¦: {imp['auc_diff']:+.4f} ({imp['auc_improvement_pct']:+.2f}%)")

        print()

    # çµ±è¨ˆã‚µãƒãƒªãƒ¼
    if comparisons:
        improvements = [c['improvements']['auc_diff'] for c in comparisons if 'improvements' in c]
        if improvements:
            print(f"å¹³å‡æ”¹å–„åº¦: {np.mean(improvements):+.4f}")
            print(f"æœ€å¤§æ”¹å–„: {np.max(improvements):+.4f}")
            print(f"æœ€å°æ”¹å–„: {np.min(improvements):+.4f}")

    # çµæœã‚’JSONã§ä¿å­˜
    os.makedirs('benchmarks', exist_ok=True)
    benchmark_path = f'benchmarks/benchmark_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(benchmark_path, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'comparisons': comparisons
        }, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœä¿å­˜: {benchmark_path}")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()
